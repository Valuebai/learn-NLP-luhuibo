{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP 答疑课 (08.04.2019)\n",
    "outline:\n",
    "+ edit distance 的 parsed_solution 程序\n",
    "+ word2vec 以及在gensim里的参数设定\n",
    "\n",
    "课程内容recap:\n",
    "+ 动态规划问题具有哪些特点？如何解析solution？\n",
    "+ 自然语言处理中为什么需要 word embedding ？\n",
    "+ word2vec 的基本原理是什么？\n",
    "\n",
    "参考资料 (optional)：\n",
    "+ solutions for the traveling-salesman-problem (formally, a np-complete problem):\n",
    "https://github.com/rohanp/travelingSalesman;\n",
    "https://www.csd.uoc.gr/~hy583/papers/ch11.pdf\n",
    "+ word2vec 的原论文（Mikolov et al. 2013）:\n",
    "https://arxiv.org/abs/1301.3781\n",
    "+ skip-gram word2vec tutorial:\n",
    "http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "+ improvements on skip-gram (hierarchical softmax & negative sampling):\n",
    "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "+ According to the authors, hierarchical softmax works better for infrequent words while negative sampling works better for frequent words and better with low dimensional vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=2**10)\n",
    "def edit_distance(string1, string2):\n",
    "    \n",
    "    if len(string1) == 0: return len(string2)\n",
    "    if len(string2) == 0: return len(string1)\n",
    "    \n",
    "    tail_s1 = string1[-1]\n",
    "    tail_s2 = string2[-1]\n",
    "    \n",
    "    candidates = [\n",
    "        (edit_distance(string1[:-1], string2) + 1, 'DEL{}'.format(tail_s1)),\n",
    "        # delete tail_s1 for string 1; edit_distance + 1 for each deletion\n",
    "        (edit_distance(string1, string2[:-1]) + 1, 'ADD{}'.format(tail_s2))\n",
    "        # add tail_s2 to string 1; edit_distance + 1 for each addition\n",
    "    ]\n",
    "    \n",
    "    if tail_s1 == tail_s2:\n",
    "        both_forward = (edit_distance(string1[:-1], string2[:-1]) + 0, '')\n",
    "        # no edits\n",
    "    else:\n",
    "        both_forward = (edit_distance(string1[:-1], string2[:-1]) + 1, \n",
    "                        'SUB {} => {}'.format(tail_s1, tail_s2))\n",
    "        # substitute tail_s2 for tail_s1; edit_distance + 1 for each substitution\n",
    "        \n",
    "    candidates.append(both_forward)\n",
    "    \n",
    "    min_distance, operation = min(candidates, key=lambda x: x[0])\n",
    "    # find the minimal edit distance among all possible operations\n",
    "    \n",
    "    solution[(string1, string2)] = operation # store solution to dictionary\n",
    "    \n",
    "    return min_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('ABCD', 'ABCDE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('A', 'A'): '',\n",
       " ('A', 'AB'): 'ADDB',\n",
       " ('A', 'ABC'): 'ADDC',\n",
       " ('A', 'ABCD'): 'ADDD',\n",
       " ('A', 'ABCDE'): 'ADDE',\n",
       " ('AB', 'A'): 'DELB',\n",
       " ('AB', 'AB'): '',\n",
       " ('AB', 'ABC'): 'ADDC',\n",
       " ('AB', 'ABCD'): 'ADDD',\n",
       " ('AB', 'ABCDE'): 'ADDE',\n",
       " ('ABC', 'A'): 'DELC',\n",
       " ('ABC', 'AB'): 'DELC',\n",
       " ('ABC', 'ABC'): '',\n",
       " ('ABC', 'ABCD'): 'ADDD',\n",
       " ('ABC', 'ABCDE'): 'ADDE',\n",
       " ('ABCD', 'A'): 'DELD',\n",
       " ('ABCD', 'AB'): 'DELD',\n",
       " ('ABCD', 'ABC'): 'DELD',\n",
       " ('ABCD', 'ABCD'): '',\n",
       " ('ABCD', 'ABCDE'): 'ADDE'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('BEIJING','NANJING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('A', 'A'): '',\n",
       " ('A', 'AB'): 'ADDB',\n",
       " ('A', 'ABC'): 'ADDC',\n",
       " ('A', 'ABCD'): 'ADDD',\n",
       " ('A', 'ABCDE'): 'ADDE',\n",
       " ('AB', 'A'): 'DELB',\n",
       " ('AB', 'AB'): '',\n",
       " ('AB', 'ABC'): 'ADDC',\n",
       " ('AB', 'ABCD'): 'ADDD',\n",
       " ('AB', 'ABCDE'): 'ADDE',\n",
       " ('ABC', 'A'): 'DELC',\n",
       " ('ABC', 'AB'): 'DELC',\n",
       " ('ABC', 'ABC'): '',\n",
       " ('ABC', 'ABCD'): 'ADDD',\n",
       " ('ABC', 'ABCDE'): 'ADDE',\n",
       " ('ABCD', 'A'): 'DELD',\n",
       " ('ABCD', 'AB'): 'DELD',\n",
       " ('ABCD', 'ABC'): 'DELD',\n",
       " ('ABCD', 'ABCD'): '',\n",
       " ('ABCD', 'ABCDE'): 'ADDE',\n",
       " ('B', 'N'): 'SUB B => N',\n",
       " ('B', 'NA'): 'ADDA',\n",
       " ('B', 'NAN'): 'ADDN',\n",
       " ('B', 'NANJ'): 'ADDJ',\n",
       " ('B', 'NANJI'): 'ADDI',\n",
       " ('B', 'NANJIN'): 'ADDN',\n",
       " ('B', 'NANJING'): 'ADDG',\n",
       " ('BE', 'N'): 'DELE',\n",
       " ('BE', 'NA'): 'SUB E => A',\n",
       " ('BE', 'NAN'): 'ADDN',\n",
       " ('BE', 'NANJ'): 'ADDJ',\n",
       " ('BE', 'NANJI'): 'ADDI',\n",
       " ('BE', 'NANJIN'): 'ADDN',\n",
       " ('BE', 'NANJING'): 'ADDG',\n",
       " ('BEI', 'N'): 'DELI',\n",
       " ('BEI', 'NA'): 'DELI',\n",
       " ('BEI', 'NAN'): 'SUB I => N',\n",
       " ('BEI', 'NANJ'): 'ADDJ',\n",
       " ('BEI', 'NANJI'): '',\n",
       " ('BEI', 'NANJIN'): 'ADDN',\n",
       " ('BEI', 'NANJING'): 'ADDG',\n",
       " ('BEIJ', 'N'): 'DELJ',\n",
       " ('BEIJ', 'NA'): 'DELJ',\n",
       " ('BEIJ', 'NAN'): 'DELJ',\n",
       " ('BEIJ', 'NANJ'): '',\n",
       " ('BEIJ', 'NANJI'): 'ADDI',\n",
       " ('BEIJ', 'NANJIN'): 'ADDN',\n",
       " ('BEIJ', 'NANJING'): 'ADDG',\n",
       " ('BEIJI', 'N'): 'DELI',\n",
       " ('BEIJI', 'NA'): 'DELI',\n",
       " ('BEIJI', 'NAN'): 'DELI',\n",
       " ('BEIJI', 'NANJ'): 'DELI',\n",
       " ('BEIJI', 'NANJI'): '',\n",
       " ('BEIJI', 'NANJIN'): 'ADDN',\n",
       " ('BEIJI', 'NANJING'): 'ADDG',\n",
       " ('BEIJIN', 'N'): '',\n",
       " ('BEIJIN', 'NA'): 'DELN',\n",
       " ('BEIJIN', 'NAN'): '',\n",
       " ('BEIJIN', 'NANJ'): 'DELN',\n",
       " ('BEIJIN', 'NANJI'): 'DELN',\n",
       " ('BEIJIN', 'NANJIN'): '',\n",
       " ('BEIJIN', 'NANJING'): 'ADDG',\n",
       " ('BEIJING', 'N'): 'DELG',\n",
       " ('BEIJING', 'NA'): 'SUB G => A',\n",
       " ('BEIJING', 'NAN'): 'DELG',\n",
       " ('BEIJING', 'NANJ'): 'DELG',\n",
       " ('BEIJING', 'NANJI'): 'DELG',\n",
       " ('BEIJING', 'NANJIN'): 'DELG',\n",
       " ('BEIJING', 'NANJING'): ''}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to parse the solutions, we need to define the exit condition (\"查表\"过程的终止条件)\n",
    "# in this case, the exit condition is\n",
    "# -- if the two strings are exactly the same, we stop editing\n",
    "\n",
    "def stop_edit(string1, string2): return string1 == string2\n",
    "\n",
    "\n",
    "# now we look for solutions in the \"solution\" dictionary\n",
    "\n",
    "def parse_solution(string1, string2, solution_dic):\n",
    "    \n",
    "    parsed_solutions = [] \n",
    "    \n",
    "    while not stop_edit(string1, string2):\n",
    "        operation = solution_dic[(string1, string2)] # \"查表\" 过程\n",
    "        \n",
    "        if 'SUB' in operation:\n",
    "            string1, string2 = string1[:-1], string2[:-1] \n",
    "            # if substitution, both forward and compare\n",
    "        elif operation == '':\n",
    "            string1, string2 = string1[:-1], string2[:-1]\n",
    "            # if no edits, both forward and compare\n",
    "        elif 'DEL' in operation:\n",
    "            string1, string2 = string1[:-1], string2 \n",
    "            # delete tail of string1 and then compare\n",
    "        elif 'ADD' in operation:\n",
    "            string1, string2 = string1, string2[:-1] \n",
    "            # delete tail of string2 and then compare\n",
    "        \n",
    "        parsed_solutions.append(operation)\n",
    "    \n",
    "    return parsed_solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', 'SUB I => N', 'SUB E => A', 'SUB B => N']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_solution('BEIJING','NANJING', solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### implementing word2vec using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train a simple Word2Vec model\n",
    "\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('say', 0.15734419226646423),\n",
       " ('dog', -0.02822110615670681),\n",
       " ('woof', -0.054968975484371185),\n",
       " ('meow', -0.09244367480278015)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('cat') # check out the most similar words for \"cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.6197851e-03, -3.7394990e-03, -1.9653954e-03,  4.9299968e-04,\n",
       "       -1.7356514e-03,  1.3853174e-03, -1.3880365e-03,  3.7120937e-03,\n",
       "        4.0574811e-04, -6.7226618e-04,  4.1654790e-03, -1.7620082e-03,\n",
       "       -2.2858770e-03, -1.2873072e-03,  1.3976595e-03,  3.2130738e-03,\n",
       "        4.0538125e-03, -2.0839639e-04,  2.0860375e-03,  1.9286850e-03,\n",
       "        4.2720833e-03,  5.3239003e-04,  1.0568922e-03, -4.6414379e-03,\n",
       "       -1.1774941e-03,  3.7658615e-03,  4.1018156e-04,  7.8001170e-04,\n",
       "        4.9522161e-03, -9.2942256e-04,  4.3550124e-03,  4.3893387e-03,\n",
       "        8.4427436e-04, -3.0914419e-03,  4.6283836e-03,  1.2586201e-03,\n",
       "       -4.6638153e-03,  2.6103409e-03, -9.8581535e-05,  4.6953690e-05,\n",
       "        2.3059270e-03,  4.2193383e-03, -3.2957387e-03,  2.6589034e-03,\n",
       "        2.5297115e-03, -4.0766471e-03, -2.6186807e-03,  1.2288799e-03,\n",
       "        1.0929701e-03,  4.7582344e-04,  4.1757482e-03, -1.0421752e-03,\n",
       "       -2.5041485e-03, -3.1426621e-03,  1.3271449e-03,  3.8764635e-03,\n",
       "        6.3386827e-04,  3.9250593e-04, -2.5958852e-03,  4.2596138e-03,\n",
       "        1.6572456e-04, -1.0934639e-03, -4.0192883e-03,  2.1248702e-03,\n",
       "       -2.5236052e-03,  2.4884060e-04,  1.5421649e-03,  1.0640332e-04,\n",
       "        1.5873985e-03,  4.9386630e-03, -1.5930380e-03,  9.7939675e-04,\n",
       "       -4.4276952e-03,  5.6074443e-04,  1.9429101e-03,  3.8868861e-04,\n",
       "        5.7578459e-04,  6.9912517e-04, -2.4010292e-04,  3.7967544e-03,\n",
       "        2.6180404e-03,  4.1164947e-03, -4.2281090e-03, -4.5641609e-03,\n",
       "       -7.8501255e-04, -2.3275891e-03, -4.2274324e-03,  3.6862968e-03,\n",
       "       -3.3401647e-03,  5.2051467e-04,  4.8682988e-03, -3.8628338e-03,\n",
       "        1.3406869e-03,  3.0320152e-04,  3.4443701e-03,  8.2645856e-04,\n",
       "        1.5438324e-03, -9.7589247e-04, -2.7780754e-03, -4.5065037e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['dog'] # check out the word vector for 'dog'\n",
    "\n",
    "# note \n",
    "# -- the default size of the world vector is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try word2vec using a larger corpus (the chinese news corpus in LineSentence format)\n",
    "\n",
    "line_setences_path = '/Users/xinweixu/Dropbox/learn/Comp_Prog/nlp/data/sentences-cut.txt'\n",
    "sentences = LineSentence(line_setences_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%time # check out the cell execution time\n",
    "news_model_1 = Word2Vec(sentences, min_count=5, size = 50)\n",
    "# notes:\n",
    "# -- ignore words appearing less than 5 times in the corpus\n",
    "# -- set the word vector size to be 50\n",
    "# -- size: the dimensionality of the vector, or the size of the neural net layers, \n",
    "#    the default is 100, and larger size would require more training data!\n",
    "\n",
    "\n",
    "# See documentation here: \n",
    "# https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model_1.window # the default window size is 5 (we used a default setting of CBOW)\n",
    "\n",
    "# -- window: how many words before and after a given word would be \n",
    "#    included as context words of the given word.\n",
    "#    According to Mikolov et al.'s notes, the recommended value is 10 for skip-gram and 5 for CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6583145 ,  0.23051035,  0.35316232, -0.12076233, -0.23267202,\n",
       "       -0.3545135 ,  0.37806377, -0.84230447,  0.19441096, -0.3814887 ,\n",
       "        0.12050633, -0.23351657, -0.0562492 ,  0.32234514,  0.36346287,\n",
       "       -0.13334295, -0.06037512, -0.23620208, -0.12063441,  0.30764708,\n",
       "        0.0938347 ,  0.03143591, -0.48876804,  0.35775042,  0.12461712,\n",
       "       -0.16470769, -0.5739129 ,  0.21065676, -0.00166393,  0.639909  ,\n",
       "        0.37221038, -0.48421142, -0.01305088,  0.32505998, -0.3972132 ,\n",
       "        0.09074134, -0.1852542 , -0.25409117,  0.27777195,  0.39134544,\n",
       "       -0.26426288,  0.2804928 , -0.33939067,  0.03918035, -0.20144188,\n",
       "       -0.14565597, -0.38240653,  0.18649364, -0.09471973, -0.16158222],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model_1.wv['霍金'] # check out the word vector for '霍金'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('外星人', 0.7982823252677917),\n",
       " ('发表文章', 0.7825572490692139),\n",
       " ('这番话', 0.737501323223114),\n",
       " ('两篇', 0.7329206466674805),\n",
       " ('凯利', 0.7296558022499084),\n",
       " ('佳佳', 0.7218890190124512),\n",
       " ('研究者', 0.720791757106781),\n",
       " ('倪光南', 0.7181127071380615),\n",
       " ('署名文章', 0.7146850228309631),\n",
       " ('半月刊', 0.7076667547225952),\n",
       " ('谢泽雄', 0.6993716955184937),\n",
       " ('心理学家', 0.6986111402511597),\n",
       " ('如是', 0.6861940026283264),\n",
       " ('引用', 0.684630274772644),\n",
       " ('乔纳森', 0.6833487153053284),\n",
       " ('李启威', 0.6798511147499084),\n",
       " ('旁人', 0.679509699344635),\n",
       " ('这篇', 0.679101824760437),\n",
       " ('华尔特', 0.6787354946136475),\n",
       " ('类人猿', 0.678704023361206)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model_1.most_similar('霍金', topn=20) # check out the top 20 most similar words to '霍金'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would the results differ if we use a different model set-up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "news_model_2 = Word2Vec(sentences, min_count=5, size = 150, window = 10, sg = 1, workers = 8)\n",
    "\n",
    "# notes:\n",
    "# -- this time we set the word vector size to be 150, context window to be 10 words,\n",
    "#    and use the skip-gram algorithm (which takes longer time!!!)\n",
    "# -- sg: training algorithm -- 1 for skip-gram; otherwise CBOW.\n",
    "# -- workers: speed up computing time using multiple cores \n",
    "#   (capped by the number of cores on your machine!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09543171,  0.36387163,  0.53725225,  0.500516  , -0.16229299,\n",
       "       -0.11686423,  0.47798058, -0.2476761 , -0.49148753, -0.32124692,\n",
       "       -0.46384513, -0.04520119, -0.04683913,  0.06340239, -0.17569564,\n",
       "       -0.09566884,  0.10246837, -0.08239821, -0.13604444,  0.12139518,\n",
       "        0.17819159, -0.04748364, -0.3483194 ,  0.00635009, -0.21259819,\n",
       "        0.01808786, -0.1372791 , -0.13192311, -0.3287806 ,  0.20427483,\n",
       "        0.05614374,  0.07496247,  0.26630557, -0.14758518,  0.24570504,\n",
       "       -0.18302245, -0.04394631, -0.30380732, -0.11139403,  0.26414317,\n",
       "        0.05811457,  0.24270004, -0.09761038,  0.38712123, -0.08975498,\n",
       "        0.55450153, -0.06434401,  0.24952   ,  0.1934675 , -0.03136092,\n",
       "       -0.01793445,  0.06577613, -0.32730475, -0.02902058,  0.749861  ,\n",
       "        0.06751074,  0.25108746,  0.07870406, -0.05638078,  0.26128837,\n",
       "        0.22157706,  0.40108484, -0.1727779 , -0.38061875,  0.80699366,\n",
       "        0.31915122, -0.00987758,  0.15248184,  0.49250683, -0.11169589,\n",
       "        0.3497526 , -0.7270232 , -0.39113155, -0.12199125,  0.679438  ,\n",
       "        0.0092144 ,  0.00920549, -0.1099605 ,  0.03964088, -0.10171082,\n",
       "       -0.21204317,  0.13756245, -0.38497093, -0.6025042 , -0.63700485,\n",
       "       -0.22398672, -0.48398837, -0.118265  ,  0.10893741, -0.34796348,\n",
       "        0.5016395 , -0.19738251,  0.3137139 , -0.14516954, -0.6057937 ,\n",
       "        0.05452532, -0.40337014, -0.21321277,  0.05591894, -0.46742606,\n",
       "       -0.13406552,  0.11802978,  0.0710381 ,  0.02449238, -0.4915963 ,\n",
       "       -0.0133589 ,  0.11022751, -0.02134604, -0.08734391,  0.13750662,\n",
       "        0.16616146,  0.19811511,  0.12381301,  0.25221297, -0.01386516,\n",
       "       -0.40949783,  0.14899515,  0.37713423, -0.05798305, -0.07498001,\n",
       "        0.06103598, -0.04318117, -0.46387005, -0.02670472,  0.23405363,\n",
       "        0.36217746, -0.13663888, -0.27205843,  0.04868685, -0.01832591,\n",
       "       -0.38938788, -0.05640595,  0.03457871,  0.02433786,  0.46261206,\n",
       "       -0.05559411, -0.24739112, -0.08405251,  0.40387523, -0.15092947,\n",
       "        0.10054912, -0.2995905 , -0.41297266,  0.36369717, -0.23718749,\n",
       "        0.28271464, -0.34929925,  0.425267  ,  0.13097689,  0.26290527],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model_2.wv['霍金']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('霍金的', 0.7316663861274719),\n",
       " ('双周刊', 0.7126336097717285),\n",
       " ('史密森', 0.7081307172775269),\n",
       " ('NASA', 0.6939337849617004),\n",
       " ('学术刊物', 0.6935392022132874),\n",
       " ('哈勃', 0.6920583248138428),\n",
       " ('美国航空航天局', 0.6904950141906738),\n",
       " ('奥斯', 0.6795668005943298),\n",
       " ('韦尔塔', 0.6710573434829712),\n",
       " ('美国国家航空航天局', 0.6707107424736023),\n",
       " ('伯杰', 0.6678393483161926),\n",
       " ('赫芬顿', 0.664324164390564),\n",
       " ('Nature', 0.6638203859329224),\n",
       " ('华尔特', 0.6630619168281555),\n",
       " ('博伊尔', 0.6625248789787292),\n",
       " ('朱清时', 0.6623486876487732),\n",
       " ('外星人', 0.6581864953041077),\n",
       " ('美国加利福尼亚大学', 0.6579184532165527),\n",
       " ('博尔顿', 0.6562376618385315),\n",
       " ('Science', 0.6560503244400024)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model_2.most_similar('霍金', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save the word vectors to a local directory for future use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we can save the model (you can change the filename to a desirable local directory)\n",
    "news_model_1.save('news_model_1')\n",
    "news_model_2.save('news_model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load a model, use the following:\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "news_model_1 = Word2Vec.load('news_model_1')\n",
    "news_model_2 = Word2Vec.load('news_model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, we can also save the word vectors for future query\n",
    "word_vectors_1 = news_model_1.wv\n",
    "word_vectors_1.save('news_model_1_word_vectors.kv')\n",
    "\n",
    "word_vectors_2 = news_model_2.wv\n",
    "word_vectors_2.save('news_model_2_word_vectors.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load word vectors from an existing file\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors_1 = KeyedVectors.load('news_model_1_word_vectors.kv')\n",
    "word_vectors_2 = KeyedVectors.load('news_model_2_word_vectors.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more info on saving word2vec models:\n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
